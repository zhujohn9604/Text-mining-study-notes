{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0. Import packages** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisite packages\n",
    "import numpy\n",
    "import scipy\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1. Read the data in** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "import csv\n",
    "with open('homework.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    raw_docs =list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [How to unnest a nested list?](https://stackoverflow.com/questions/11860476/how-to-unnest-a-nested-list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An apparatus and a method for diagnosis are provided. The apparatus for diagnosis lesion include: a model generation unit configured to categorize learning data into one or more categories and to generate one or more categorized diagnostic models based on the categorized learning data, a model selection unit configured to select one or more diagnostic model for diagnosing a lesion from the categorized diagnostic models, and a diagnosis unit configured to diagnose the lesion based on image data of the lesion and the selected one or more diagnostic model.',\n",
       " 'Embodiments are disclosed to provide the prediction of viewable events. Predicting viewable events will allow users to know what events will likely be viewable in a particular venue, such as a restaurant, bar, or private home. Information about venues and events is populated in a database by a plurality of venues or users. Users wishing to view a particular event can search for a venue that has a high probability of showing that event.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at first two records\n",
    "raw_docs = sum(raw_docs, [])\n",
    "raw_docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of documents\n",
    "len(raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2. Tokenization** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in raw_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3. Create a dictionary**\n",
    "\n",
    "note: a dictionary maps every word to a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "563"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dictionary\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "# Number of words in the dictionary\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ,\n",
      "1 .\n",
      "2 :\n",
      "3 a\n",
      "4 an\n",
      "5 and\n",
      "6 apparatus\n",
      "7 are\n",
      "8 based\n",
      "9 categories\n"
     ]
    }
   ],
   "source": [
    "# show first five words in the dictionary\n",
    "for i in range(10):\n",
    "    print(i, dictionary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4. Create a corpus**\n",
    "\n",
    "note: A corpus is a list of bags of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "\n",
      "\n",
      "[(0, 3), (1, 4), (3, 7), (5, 1), (7, 1), (18, 1), (31, 3), (34, 2), (39, 1), (40, 3), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 2), (53, 4), (54, 1), (55, 1), (56, 1), (57, 2), (58, 1), (59, 1), (60, 1), (61, 1), (62, 2), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 2), (75, 3), (76, 2), (77, 2), (78, 1), (79, 3), (80, 1), (81, 2), (82, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of bags of words\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(type(corpus))\n",
    "print('\\n')\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n"
     ]
    }
   ],
   "source": [
    "# we could check the corresponding words by\n",
    "print(dictionary[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5. Create a tf-idf model**\n",
    "\n",
    "note: Learn more about [tf-idf](http://www.tfidf.com/)\n",
    "\n",
    "Besides, other methods could be tried here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=25, num_nnz=1246)\n"
     ]
    }
   ],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)\n",
    "# num_nzz refers to the number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5. Similarity measures / Similarity Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.similarities.docsim.MatrixSimilarity at 0x1bf58ba4a8>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = gensim.similarities.MatrixSimilarity(tf_idf[corpus])\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6. Similarity interface**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0035008157911770176), (4, 0.042894504746844565), (18, 0.0053671438020382375), (20, 0.07694178858551294), (27, 0.02434395030301869)]\n"
     ]
    }
   ],
   "source": [
    "query_doc = [w.lower() for w in word_tokenize(raw_docs[5])]\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "query_doc_bow_tf_idf = tf_idf[query_doc_bow]\n",
    "# show the part of tf-idf weights of the queried doc\n",
    "print(query_doc_bow_tf_idf[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 1.0), (24, 0.13637403), (11, 0.07579642), (7, 0.058889322), (12, 0.048985645), (8, 0.04739786), (10, 0.041341662), (6, 0.03414415), (17, 0.033516787), (1, 0.031384718), (0, 0.03130289), (15, 0.02824454), (18, 0.025812946), (9, 0.024417594), (19, 0.023246385), (14, 0.020976612), (3, 0.02051758), (21, 0.015337167), (13, 0.013489934), (22, 0.012365733), (16, 0.0087839635), (2, 0.0067955498), (23, 0.006663728), (4, 0.0064896205), (20, 0.003776135)]\n"
     ]
    }
   ],
   "source": [
    "# query\n",
    "sims = index[query_doc_bow_tf_idf]\n",
    "# sort \n",
    "sims_sorted = sorted(enumerate(sims), key = lambda item: -item[1])\n",
    "print(sims_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A candidate message chatbot system and method. The chatbot includes an interactive dialog interface for engaging in a chat session with a user. The user can enter one or more characters as an input message during the chat session. The chatbot can match the one or more characters with a plurality of candidate messages in a knowledge database, the plurality of candidate input messages being part of input/output knowledge entry or known to generate at least a quality response.\n",
      "\n",
      "\n",
      "Method, system, and computer program product to analyze a plurality of candidate answers identified as responsive to a question presented to a deep question answering system, by computing a first feature score for a first feature of an item of evidence, of a plurality of items of evidence, the first feature score being based on at least one attribute of the first feature, the item of evidence relating to a first candidate answer, of the plurality of candidate answers, and computing a merged feature score for the first candidate answer by applying the first feature score to a second feature score for a second feature of the item of evidence.\n"
     ]
    }
   ],
   "source": [
    "# comparison\n",
    "print(raw_docs[5])\n",
    "print('\\n')\n",
    "print(raw_docs[24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* [Tutorials on gensim](https://radimrehurek.com/gensim/tutorial.html)\n",
    "\n",
    "* [How do I compare document similarity using Python?](https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
