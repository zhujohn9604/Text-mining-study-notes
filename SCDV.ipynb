{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['this', 'is', 'the', 'good', 'machine', 'learning', 'book'],\n",
    "            ['this', 'is',  'another', 'book'],\n",
    "            ['one', 'more', 'book'],\n",
    "            ['this', 'is', 'the', 'new', 'post'],\n",
    "            ['this', 'is', 'about', 'machine', 'learning', 'post'],  \n",
    "            ['and', 'this', 'is', 'the', 'last', 'post']]\n",
    "model = Word2Vec(sentences, min_count=1, size = 5, sg = 1) # based on skip-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Build Clusters -- remember to change the directories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef drange(start, stop, step):\\n    r = start\\n    while r < stop:\\n        yield r\\n        r += step\\n'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def drange(start, stop, step):\n",
    "    r = start\n",
    "    while r < stop:\n",
    "        yield r\n",
    "        r += step\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_GMM(num_clusters, word_vectors):\n",
    "    start = time.time()\n",
    "    # Initalize a GMM object and use it for clustering.\n",
    "    clf =  GaussianMixture(n_components=num_clusters,\n",
    "                    covariance_type=\"tied\", init_params='kmeans', max_iter=50)\n",
    "    # Get cluster assignments.\n",
    "    clf.fit(word_vectors)\n",
    "    idx = clf.predict(word_vectors)\n",
    "    print (\"Clustering Done...\", time.time()-start, \"seconds\")\n",
    "    # Get probabilities of cluster assignments.\n",
    "    idx_proba = clf.predict_proba(word_vectors)\n",
    "    # Dump cluster assignments and probability of cluster assignments. \n",
    "    pickle.dump(idx, open('C:/Users/zhuchen/Desktop/motohashi seminar/Word2Vec Learning/gmm_latestclusmodel_len2alldata.pkl',\"wb\"))\n",
    "    print (\"Cluster Assignments Saved...\")\n",
    "\n",
    "    pickle.dump(idx_proba,open( 'C:/Users/zhuchen/Desktop/motohashi seminar/Word2Vec Learning/gmm_prob_latestclusmodel_len2alldata.pkl',\"wb\"))\n",
    "    print (\"Probabilities of Cluster Assignments Saved...\")\n",
    "    return (idx, idx_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_GMM(idx_name, idx_proba_name):\n",
    "    # Loads cluster assignments and probability of cluster assignments. \n",
    "    idx = pickle.load(open('C:/Users/zhuchen/Desktop/motohashi seminar/Word2Vec Learning/gmm_latestclusmodel_len2alldata.pkl',\"rb\"))\n",
    "    idx_proba = pickle.load(open( 'C:/Users/zhuchen/Desktop/motohashi seminar/Word2Vec Learning/gmm_prob_latestclusmodel_len2alldata.pkl',\"rb\"))\n",
    "    print (\"Cluster Model Loaded...\")\n",
    "    return (idx, idx_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Done... 0.7695479393005371 seconds\n",
      "Cluster Assignments Saved...\n",
      "Probabilities of Cluster Assignments Saved...\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 2\n",
    "(idx, idx_proba) = cluster_GMM(num_clusters, word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_centroid_map = dict(zip(model.wv.index2word, idx))\n",
    "word_centroid_prob_map = dict(zip(model.wv.index2word, idx_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is the good machine learning book',\n",
       " 'this is another book',\n",
       " 'one more book',\n",
       " 'this is the new post',\n",
       " 'this is about machine learning post',\n",
       " 'and this is the last post']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_idf_ready = [' '.join(sentence) for sentence in sentences]\n",
    "sentences_idf_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer()\n",
    "tfidfmatrix = tfv.fit_transform(sentences_idf_ready)\n",
    "featurenames = tfv.get_feature_names()\n",
    "idf = tfv._tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idf_dict = dict(zip(featurenames, idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_word_vectors(model, featurenames, word_centroid_map, word_centroid_prob_map, word_idf_dict, num_features, num_clusters):\n",
    "    # This function computes probability word-cluster vectors.\n",
    "    # word_centroid_map: (word, cluster_id)\n",
    "    # word_centroid_prob_map: (word, cluster_prob)\n",
    "    prob_wordvecs = {}\n",
    "    \n",
    "    for word in word_centroid_map:\n",
    "        prob_wordvecs[word] = np.zeros( num_clusters * num_features, dtype=\"float32\" )\n",
    "        for index in range(0, num_clusters):\n",
    "            try:\n",
    "                prob_wordvecs[word][index*num_features:(index+1)*num_features] = model.wv[word] * word_centroid_prob_map[word][index] * word_idf_dict[word]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "    return prob_wordvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 2\n",
    "num_features = 5\n",
    "prob_wordvecs = get_probability_word_vectors(model, featurenames, word_centroid_map, word_centroid_prob_map, word_idf_dict, num_features, num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_vector_and_gwbowv(prob_wordvecs, wordlist, word_centroid_map, word_centroid_prob_map, dimension, word_idf_dict, featurenames, num_centroids, train=False):\n",
    "    # This function computes SDV feature vectors.\n",
    "    bag_of_centroids = np.zeros( num_centroids * dimension, dtype=\"float32\" )\n",
    "    global min_no\n",
    "    global max_no\n",
    "\n",
    "    # wordlist : a document consists of a list of tokens\n",
    "    for word in wordlist:\n",
    "        try:\n",
    "            temp = word_centroid_map[word]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        bag_of_centroids += prob_wordvecs[word]\n",
    "        \n",
    "    norm = np.sqrt(np.einsum('...i,...i', bag_of_centroids, bag_of_centroids))\n",
    "    if(norm!=0):\n",
    "        bag_of_centroids /= norm\n",
    "\n",
    "    # To make feature vector sparse, make note of minimum and maximum values.\n",
    "    if train:\n",
    "        min_no += min(bag_of_centroids)\n",
    "        max_no += max(bag_of_centroids)\n",
    "\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build SCDV features\n",
    "num_clusters = 2\n",
    "num_features = 5\n",
    "dimension = num_features\n",
    "num_centroids = num_clusters\n",
    "# sparsity setting\n",
    "min_no = 0\n",
    "max_no = 0\n",
    "# run\n",
    "doc_scdv = [create_cluster_vector_and_gwbowv(prob_wordvecs, doc, word_centroid_map, word_centroid_prob_map, dimension, word_idf_dict, featurenames, num_centroids, train=True) \\\n",
    " for doc in sentences]\n",
    "doc_scdv = np.vstack(doc_scdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Average min:  -0.558233305811882\n",
      "Averge max:  0.4659252464771271\n"
     ]
    }
   ],
   "source": [
    "# Set the threshold percentage for making it sparse\n",
    "percentage = 0.04\n",
    "print(len(doc_scdv))\n",
    "min_no = min_no*1.0/len(doc_scdv)\n",
    "max_no = max_no*1.0/len(doc_scdv)\n",
    "print('Average min: ', min_no)\n",
    "print('Averge max: ', max_no)\n",
    "threshold = (abs(min_no) + abs(max_no))/2\n",
    "threshold = threshold * percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3607643 ,  0.2232363 ,  0.34561348,  0.5220664 , -0.5360979 ,\n",
       "         0.        ,  0.24476816,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.5010719 , -0.6639076 , -0.26762426, -0.43843392, -0.21049167,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.36032736, -0.24551527, -0.26580805,  0.57196534,  0.641941  ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.29085016,  0.        , -0.21184976,  0.        , -0.7502883 ,\n",
       "         0.        ,  0.3337964 ,  0.        ,  0.30128208, -0.24067657],\n",
       "       [ 0.        ,  0.        ,  0.26648086,  0.35002476, -0.38150784,\n",
       "         0.28476605,  0.3804079 , -0.21408658,  0.46766   , -0.40083453],\n",
       "       [ 0.        , -0.29343304,  0.        , -0.7324635 , -0.23919412,\n",
       "         0.        ,  0.32901576,  0.        ,  0.2969671 , -0.23722959]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = abs(doc_scdv) < threshold\n",
    "doc_scdv[temp] = 0\n",
    "doc_scdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it\n",
    "#np.save(\"../japanese-dataset/livedoor-news-corpus/model/\"+gwbowv_name, gwbowv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
